import { motion } from "framer-motion";
import { Mic, MicOff } from "lucide-react";
import { useState, useEffect, useRef } from "react";

interface VoiceControlProps {
  onSpeechRecognized: (text: string) => void;
  isSpeaking: boolean;
  currentMessage: string;
  gender?: "female" | "male";
  language?: "fr-FR" | "en-US";
  selectedLanguage?: "fr-FR" | "en-US";
}

/**
 * üéôÔ∏è VoiceControl ‚Äì Z√âNA QVT Box
 * -----------------------------------------------------------
 * - Reconnaissance vocale r√©elle (Web Speech API)
 * - Halo anim√© QVT Box turquoise/violet
 * - Texte capt√© en direct
 * - Design doux, professionnel et fluide
 */
export default function VoiceControl({
  onSpeechRecognized,
  isSpeaking,
  currentMessage,
  gender = "female",
  language = "fr-FR",
  selectedLanguage = "fr-FR",
}: VoiceControlProps) {
  const [isListening, setIsListening] = useState(false);
  const [transcript, setTranscript] = useState("");
  const recognitionRef = useRef<SpeechRecognition | null>(null);

  // üé§ Initialisation du micro (Web Speech API)
  useEffect(() => {
    const SpeechRecognition =
      (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition;

    if (!SpeechRecognition) {
      console.warn("‚ö†Ô∏è Web Speech API non support√©e sur ce navigateur.");
      return;
    }

    const recognition = new SpeechRecognition();
    recognition.lang = language;
    recognition.continuous = true; // Chang√© en true pour mobile
    recognition.interimResults = true;
    recognition.maxAlternatives = 1;
    
    console.log("üé§ Reconnaissance vocale initialis√©e avec la langue:", language);

    recognition.onresult = (event: SpeechRecognitionEvent) => {
      let newTranscript = "";
      for (let i = event.resultIndex; i < event.results.length; ++i) {
        newTranscript += event.results[i][0].transcript;
      }
      setTranscript(newTranscript);
      
      // N'envoyer que les r√©sultats finaux (quand l'utilisateur a fini de parler)
      if (event.results[event.results.length - 1].isFinal && newTranscript.trim()) {
        console.log("‚úÖ Texte final capt√©:", newTranscript.trim());
        onSpeechRecognized(newTranscript.trim());
        // Arr√™ter apr√®s avoir re√ßu un r√©sultat final
        recognition.stop();
      }
    };

    recognition.onerror = (event: any) => {
      console.error("‚ùå Erreur SpeechRecognition :", event.error);
      const errorMessages: { [key: string]: string } = {
        "not-allowed": selectedLanguage === "fr-FR" ? "Permission du microphone refus√©e" : "Microphone permission denied",
        "no-speech": selectedLanguage === "fr-FR" ? "Aucune parole d√©tect√©e" : "No speech detected",
        "audio-capture": selectedLanguage === "fr-FR" ? "Microphone non disponible" : "Microphone not available",
        "network": selectedLanguage === "fr-FR" ? "Erreur r√©seau" : "Network error"
      };
      
      if (errorMessages[event.error]) {
        alert(errorMessages[event.error]);
      }
      setIsListening(false);
    };

    recognition.onend = () => {
      console.log("üî¥ Reconnaissance vocale arr√™t√©e");
      setIsListening(false);
    };

    recognitionRef.current = recognition;
  }, [onSpeechRecognized, language, selectedLanguage]);

  // üöÄ Actions : d√©marrer / arr√™ter
  const startListening = async () => {
    if (!recognitionRef.current) {
      console.error("‚ùå Reconnaissance vocale non disponible");
      alert(selectedLanguage === "fr-FR" 
        ? "‚ùå La reconnaissance vocale n'est pas disponible sur ce navigateur. Essayez Chrome sur Android."
        : "‚ùå Speech recognition is not available on this browser. Try Chrome on Android.");
      return;
    }
    
    try {
      // V√©rifier si le micro est disponible
      if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
        throw new Error("getUserMedia not supported");
      }

      // Demander explicitement les permissions du microphone avec contraintes mobiles
      console.log("üé§ Demande de permission du microphone...");
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        } 
      });
      console.log("‚úÖ Permission du microphone accord√©e");
      
      // Garder le stream actif pendant la reconnaissance
      const audioContext = new AudioContext();
      const source = audioContext.createMediaStreamSource(stream);
      source.connect(audioContext.destination);
      
      // D√©marrer la reconnaissance
      recognitionRef.current.start();
      setIsListening(true);
      setTranscript("");
      console.log("üé§ Reconnaissance vocale d√©marr√©e");
      
      // Lib√©rer le stream apr√®s quelques secondes
      setTimeout(() => {
        stream.getTracks().forEach(track => track.stop());
        audioContext.close();
      }, 30000); // 30 secondes max
      
    } catch (err: any) {
      console.error("‚ùå Erreur d√©marrage micro :", err);
      let errorMessage = selectedLanguage === "fr-FR"
        ? "‚ùå Impossible d'acc√©der au microphone.\n\n"
        : "‚ùå Cannot access microphone.\n\n";
      
      if (err.name === "NotAllowedError" || err.name === "PermissionDeniedError") {
        errorMessage += selectedLanguage === "fr-FR"
          ? "Veuillez autoriser l'acc√®s au microphone dans les param√®tres de votre navigateur."
          : "Please allow microphone access in your browser settings.";
      } else if (err.name === "NotFoundError") {
        errorMessage += selectedLanguage === "fr-FR"
          ? "Aucun microphone trouv√© sur votre appareil."
          : "No microphone found on your device.";
      } else {
        errorMessage += selectedLanguage === "fr-FR"
          ? `Erreur: ${err.message || 'Permission refus√©e'}\n\nEssayez Chrome sur Android pour une meilleure compatibilit√©.`
          : `Error: ${err.message || 'Permission denied'}\n\nTry Chrome on Android for better compatibility.`;
      }
      
      alert(errorMessage);
    }
  };

  const stopListening = () => {
    recognitionRef.current?.stop();
    setIsListening(false);
  };

  // üåà Couleur du halo selon le genre
  const auraColor =
    gender === "female"
      ? "from-[#5B4B8A]/60 to-[#4FD1C5]/40"
      : "from-[#4FD1C5]/60 to-[#5B4B8A]/40";

  return (
    <div className="flex flex-col items-center justify-center gap-4 w-full text-center">
      {/* ==== HALO ANIM√â ==== */}
      <div className="relative flex items-center justify-center">
        {/* Halo pulsant */}
        <motion.div
          className={`absolute w-32 h-32 md:w-40 md:h-40 rounded-full blur-3xl bg-gradient-to-br ${auraColor}`}
          animate={{
            scale: isListening ? [1, 1.2, 1] : [1, 1.05, 1],
            opacity: isListening ? [0.7, 1, 0.8] : [0.3, 0.5, 0.3],
          }}
          transition={{ duration: 2, repeat: Infinity }}
        />

        {/* Onde visuelle */}
        {isListening && (
          <motion.div
            className="absolute rounded-full border-2 border-[#4FD1C5]/50 w-24 h-24 md:w-32 md:h-32"
            animate={{
              scale: [1, 1.4, 1],
              opacity: [0.8, 0.1, 0.8],
            }}
            transition={{ duration: 2, repeat: Infinity }}
          />
        )}

        {/* Micro bouton */}
        <motion.button
          onClick={isListening ? stopListening : startListening}
          className={`relative z-10 w-16 h-16 md:w-20 md:h-20 flex items-center justify-center rounded-full shadow-lg transition-all focus:outline-none 
            ${
              isListening
                ? "bg-gradient-to-r from-[#005B5F] to-[#4FD1C5] text-white"
                : "bg-white text-[#005B5F] border border-[#4FD1C5]/40"
            }`}
          animate={{
            scale: isListening ? [1, 1.05, 1] : [1, 0.98, 1],
          }}
          transition={{ duration: 1.5, repeat: Infinity }}
          aria-label={isListening ? "Arr√™ter l‚Äô√©coute" : "D√©marrer l‚Äô√©coute"}
        >
          {isListening ? <MicOff size={32} /> : <Mic size={32} />}
        </motion.button>
      </div>

      {/* ==== TRANSCRIPTION EN DIRECT ==== */}
      <motion.div
        className="w-full max-w-md min-h-[50px] mt-4 px-4 py-2 bg-white/70 rounded-2xl shadow-inner text-sm text-[#212121]/80 border border-[#EAF4F3]/80"
        initial={{ opacity: 0, y: 10 }}
        animate={{ opacity: 1, y: 0 }}
      >
        {transcript || currentMessage ? (
          <p className="leading-relaxed">
            {transcript || currentMessage}
          </p>
        ) : (
          <p className="italic text-gray-400">Votre voix s‚Äôaffichera ici...</p>
        )}
      </motion.div>

      {/* ==== √âTAT DU MICRO ==== */}
      <p className="text-xs text-gray-500 mt-1">
        {isSpeaking
          ? "üîä Z√âNA parle..."
          : isListening
          ? "üéß Z√âNA vous √©coute..."
          : "Appuyez sur le micro pour parler"}
      </p>
    </div>
  );
}
